# -*- coding: utf-8 -*-
"""Scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18GfaHtYvugzfSCKHJv-xnadb4bUhbMVr
"""

import requests
import pandas as pd
import time

# Google API details (replace with your actual credentials)
API_KEY = ""
CSE_ID = ""
GOOGLE_SEARCH_URL = "https://www.googleapis.com/customsearch/v1"

# Target roles
TARGET_ROLES = [
    "Lab Director", "Principal Investigator", "R&D Manager",
    "Manufacturing Engineer", "Process Development Engineer", "Technical Buyer"
]

# Focus organizations (used to refine searches)
FOCUS_ORGANIZATIONS = [
    "Research university", "Biotech company", "Semiconductor manufacturer",
    "Advanced manufacturing facility", "Security contractor", "Defense contractor"
]

# Function to search LinkedIn profiles using Google Custom Search API
def linkedin_search(role, organization, num_results=10):
    """Search LinkedIn profiles based on job roles and organizations."""

    search_query = f'site:linkedin.com/in/ "{role}" "{organization}"'

    params = {
        "q": search_query,
        "key": API_KEY,
        "cx": CSE_ID,
        "num": num_results,
    }

    response = requests.get(GOOGLE_SEARCH_URL, params=params)

    if response.status_code == 200:
        return response.json().get("items", [])
    else:
        print(f"Error: {response.status_code}, {response.text}")
        return None

# Function to classify application area and priority level
def classify_profile(title, organization):
    """Classifies the application area and priority level based on job title and organization."""
    if "biotech" in organization.lower():
        application_area = "Biotech"
    elif "semiconductor" in organization.lower():
        application_area = "Semiconductors"
    elif "manufacturing" in organization.lower():
        application_area = "Advanced Manufacturing"
    elif "security" in organization.lower() or "defense" in organization.lower():
        application_area = "Security/Defense"
    else:
        application_area = "General Research"

    priority_level = "High" if any(role.lower() in title.lower() for role in TARGET_ROLES) else "Medium"

    return application_area, priority_level

# Main function to scrape and store data
def scrape_linkedin_profiles():
    all_results = []

    for role in TARGET_ROLES:
        for organization in FOCUS_ORGANIZATIONS:
            results = linkedin_search(role, organization, num_results=10)
            if results:
                for result in results:

                    title = result.get("title", "Unknown")
                    link = result.get("link", "")
                    snippet = result.get("snippet", "")

                    # Splitting at the hyphen (-) to separate Name, Title, and Company
                    parts = title.split(" - ")

                    if len(parts) == 3:  # Expected format: Name - Title - Company
                        name, job_title, company = parts
                    elif len(parts) == 2:  # If company is missing
                        name, job_title = parts
                        company = "Unknown"
                    else:  # If splitting fails, assign defaults
                        name = title
                        job_title = "Unknown"
                        company = "Unknown"

                    # Trim spaces to clean up the data
                    name, job_title, company = name.strip(), job_title.strip(), company.strip()
                    if company.endswith("| LinkedIn"):
                        company = company.replace("| LinkedIn", "").strip()

                    # Classify application area and priority level
                    application_area, priority_level = classify_profile(job_title, company)

                    # Append to results
                    all_results.append([name, job_title, company, link, application_area, priority_level])




                    # Avoid hitting Google API rate limits
                    time.sleep(1)

            # Stop when we have 50 contacts
            if len(all_results) >= 50:
                break

    # Convert to DataFrame
    df = pd.DataFrame(all_results, columns=["Name", "Title", "Company", "LinkedIn URL", "Application Area", "Priority Level"])

    # Save to Excel
    file_path = "LinkedIn_Contacts.xlsx"
    df.to_excel(file_path, index=False)

    print(f"Saved {len(df)} contacts to {file_path}")

    return df

# Run script
if __name__ == "__main__":
    df = scrape_linkedin_profiles()
